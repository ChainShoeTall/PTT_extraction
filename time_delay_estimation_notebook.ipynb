{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Resolution Time Delay Estimation Between Noisy Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Domain Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Correlation and Parabolic Peak Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-correlation between two signals $x(t)$ and $y(t)$ (with $y$ being a delayed version of $x$ plus noise)\n",
    "\n",
    "which, in discrete form, becomes a sum over time shifts. The delay that maximizes $R_{xy}(\\tau)$ is the estimated time difference of arrival. In practice, one computes the discrete cross-correlation (using xcorr or FFT methods) and finds the index $k_0$ of the peak. This $k_0$ gives the nearest integer-sample delay. To achieve fractional delay estimates, we can interpolate around this peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y: input signals (1D arrays), assume len(y) ≈ len(x)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.rPPG_methods import CHROM_win\n",
    "\n",
    "pkl_path = \"data/example/highspeed_roi/sid101-rid1-qhy5iii174c-seg1-roi.pkl\"\n",
    "def load_xy_from_pkl(pkl_path, hs_fps=150):\n",
    "    roi_df = np.load(pkl_path, allow_pickle=True)\n",
    "    x = roi_df['roi-forehead'].to_list()\n",
    "    y = roi_df['roi-right_cheek'].to_list()\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    x = x[:,[2,1,0]] # BGR -> RGB\n",
    "    y = y[:,[2,1,0]]\n",
    "\n",
    "    x = CHROM_win(x[:,None,:], FS=hs_fps)\n",
    "    y = CHROM_win(y[:,None,:], FS=hs_fps)\n",
    "    x = x[0]\n",
    "    y = y[0]\n",
    "    return x, y\n",
    "x, y = load_xy_from_pkl(pkl_path)\n",
    "hs_fps = 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parabolic Peak Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach is parabolic interpolation using the correlation values at the peak and its immediate neighbors. Assuming $R_{xy}(k_0)$ is the maximum at integer lag $k_0$, with neighbors $R_{xy}(k_0-1)$ and $R_{xy}(k_0+1)$, we fit a quadratic through these three points and find its vertex. If we denote $y(-1)=R_{xy}(k_0-1)$, $y(0)=R_{xy}(k_0)$, $y(1)=R_{xy}(k_0+1)$, the fractional offset $\\delta$ from $k_0$ that maximizes the parabola is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_delay(x, y):\n",
    "    # Compute cross-correlation (full mode to get negative and positive lags)\n",
    "    corr = np.correlate(x, y, mode='full')  \n",
    "    lags = np.arange(-len(y)+1, len(x))    # corresponding lag values\n",
    "\n",
    "    # Find index of max correlation\n",
    "    k0 = np.argmax(corr)\n",
    "    peak_lag = lags[k0]        # integer lag of max correlation\n",
    "\n",
    "    # Parabolic interpolation around the peak\n",
    "    if 0 < k0 < len(corr)-1:\n",
    "        y_m1, y_0, y_p1 = corr[k0-1], corr[k0], corr[k0+1]\n",
    "        denom = (y_m1 - 2*y_0 + y_p1)\n",
    "        if denom != 0:\n",
    "            delta = (y_m1 - y_p1) / (2 * denom)\n",
    "        else:\n",
    "            delta = 0.0\n",
    "    else:\n",
    "        delta = 0.0\n",
    "    frac_delay = peak_lag + delta   # estimated fractional delay (in samples)\n",
    "    return frac_delay\n",
    "frac_delay = frac_delay(x, y)\n",
    "print(f\"Fractional delay: {frac_delay*1000/hs_fps:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinc Interpolation and Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than fitting a simple parabola, one can more precisely model the continuous cross-correlation function using band-limited interpolation. If the signals are band-limited (which is often the case if they were sampled at Nyquist rate), the cross-correlation will be a band-limited function. Sinc interpolation (reconstructing the correlation with an ideal low-pass filter) can yield an accurate fractional-delay estimate with minimal bias. In practice, ideal sinc reconstruction is not feasible, but one can **upsample** the signals or the correlation via zero-padding in frequency domain to approximate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Time-domain upsampling**: Increase the sampling rate (e.g., by polyphase filtering or inserting zeros and lowpass filtering) for one or both signals and then compute cross-correlation at the higher rate. With a high enough upsampling factor, the delay can be determined to the desired fraction of a sample. This is computationally more expensive (since signals length and correlation computations increase by the upsampling factor).\n",
    "- **Frequency-domain zero-padding**: A clever trick is to compute the cross-correlation via FFT with zero-padding to achieve interpolation. For example, take FFTs of $x$ and $y$ (of length $N$), zero-pad them to length $M \\gg N$, then compute the cross-correlation via inverse FFT of $X(f)Y^*(f)$. The resulting cross-correlation is effectively sampled at a finer time grid (because zero-padding in frequency corresponds to interpolation in time). This yields a dense correlation curve from which a more precise peak can be found. Note: This is equivalent to sinc interpolation of the correlation function.\n",
    "- **Gaussian or Cosine interpolation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinc_interp(x, y, scale_factor:int=10):\n",
    "    N = len(x)\n",
    "    M = scale_factor * N  # e.g., 16x upsampling in time\n",
    "    X = np.fft.rfft(x, n=N)\n",
    "    Y = np.fft.rfft(y, n=N)\n",
    "    # Cross-spectrum and normalized inverse FFT\n",
    "    cross_spec = X * np.conj(Y)\n",
    "\n",
    "    # zero-pad them to M in frequency domain\n",
    "    cross_spec = np.pad(cross_spec, (0, M//2-N//2), constant_values=(0,))\n",
    "    corr = np.fft.irfft(cross_spec, n=M)  # length M correlation, contains correlation at finer steps\n",
    "    # Because signals are of length N, valid correlation lags are within +/- (N-1)\n",
    "    # corr array is circular; we should shift it to center = zero lag\n",
    "    corr = np.roll(corr, M//2)  \n",
    "    valid_lags = np.arange(-N+1, N)\n",
    "    corr_valid = corr[M//2 - (N-1) : M//2 + (N-1) + 1]\n",
    "    k0 = np.argmax(corr_valid)\n",
    "    peak_lag_frac = valid_lags[0] + k0\n",
    "\n",
    "    return peak_lag_frac/scale_factor\n",
    "\n",
    "sinc_interp_factor = 10\n",
    "peak_lag_frac = sinc_interp(x, y, scale_factor=sinc_interp_factor)\n",
    "print(f\"Sinc interpolation delay: {peak_lag_frac*1000/hs_fps:.3f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency-Domain Techniques\n",
    "Frequency-domain methods exploit the fact that a time delay corresponds to a linear phase shift across frequency components. If $y(t)$ is a delayed version of $x(t)$ by $\\tau$, and ignoring noise for the moment, their continuous Fourier transforms are related by $Y(f) = X(f),e^{-j 2\\pi f \\tau}$. This means the cross-power spectrum $S_{xy}(f) = X(f) , Y^*(f)$ contains a phase term $e^{-j2\\pi f \\tau}$ proportional to the delay. By analyzing the phase of the cross-spectrum (or the transfer function between $x$ and $y$), one can estimate $\\tau$ with fine resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Difference (Cross-Spectrum) Method (NOT WORKING WELL!)\n",
    "\n",
    "In practice, given discrete signals, one can compute the cross-spectrum via the FFT. Let $X[k]$ and $Y[k]$ be the DFTs of $x$ and $y$. The cross-spectrum is $C[k] = X[k]\\,Y^*[k]$. If $y$ is a shifted version of $x$, we expect:\n",
    "\n",
    "$$C[k] \\approx A(k)\\,e^{-j\\omega_k \\tau},$$\n",
    "\n",
    "where $\\omega_k = 2\\pi k / N$ is the digital frequency and $A(k)$ is some complex amplitude (including any gain differences). The phase of $C[k]$ is $\\angle C[k] \\approx -\\omega_k \\tau$ (mod $2\\pi$). If the delay $\\tau$ is small, this phase is nearly linear in $k$. We can unwrap the phase across the spectrum and perform a linear regression: the slope of $\\angle C(\\omega)$ vs. $\\omega$ gives $-\\tau$. In other words,\n",
    "\t•\tCompute $\\phi[k] = \\text{unwrap}(\\angle C[k])$ for $k=0…N-1$.\n",
    "\t•\tFit a line $\\phi[k] \\approx -\\omega_k \\tau + b$ (where $b$ is some constant phase offset). The best-fit slope in a least-squares sense yields $\\hat{\\tau}$.\n",
    "\n",
    "Another equivalent view: the time delay can be estimated by the group delay between the signals, defined as $-\\frac{d}{d\\omega}\\angle C(\\omega)$. For a pure delay, this is constant and equal to the delay. In discrete form, one can compute group delay by finite difference of phase or by special algorithms.\n",
    "\n",
    "A simple example: if $x[n]$ and $y[n]$ are sinusoids of frequency $f_0$ with a phase difference, the delay can be obtained from that phase difference divided by $2\\pi f_0$. For broadband signals, the phase method uses all frequencies to improve accuracy (basically a larger aperture for estimation).\n",
    "\n",
    "**Why it fails:**\n",
    "- Phase unwrapping and ambiguity: One must unwrap the phase correctly. If the delay is large such that phase changes by more than $2\\pi$ over the band, naive unwrapping might jump by $2\\pi$ at wrong places. However, this is usually manageable by standard unwrapping algorithms if SNR is reasonably high.\n",
    "- Requires coherent spectrum: If the two signals differ not just by delay but also by other filtering (e.g., different frequency responses or multipath effects), the phase won’t be a perfect line. That can bias the estimate unless one restricts to frequency regions where the phase is linear. In other words, this assumes the two signals are related by a pure delay (and maybe gain); any deviation (like distortion) reduces accuracy.\n",
    "- Sensitive to low SNR at certain frequencies: A frequency bin with little signal (and mostly noise) will have a random phase, which can throw off the linear fit. Typically one must weight or discard frequencies with low coherence. Using a cross-spectrum coherence threshold or weighting by $|C[k]|$ can mitigate this.\n",
    "- Implementation complexity is moderate: fitting a line to phase data is straightforward, but one must handle edge cases (phase wrap, outliers) carefully. It’s more complex than a simple time-domain peak search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fs = hs_fps\n",
    "N = len(x)\n",
    "X = np.fft.rfft(x)\n",
    "Y = np.fft.rfft(y)\n",
    "# Assume we have cross-spectrum from earlier FFTs: cross_spec = X * np.conj(Y)\n",
    "cross_spec = X * np.conj(Y)  # from previous example or computed elsewhere\n",
    "freqs = np.fft.rfftfreq(N, d=1.0/fs)  # frequency axis (Fs = sampling rate)\n",
    "phase = np.angle(cross_spec)\n",
    "phase_unwrapped = np.unwrap(phase)\n",
    "\n",
    "# Optionally select frequency range of interest:\n",
    "# e.g., use only frequencies where |cross_spec| is significant \n",
    "mask = np.abs(cross_spec) > np.max(np.abs(cross_spec)) * 0.1  # example threshold\n",
    "freqs_used = freqs[mask]\n",
    "phase_used = phase_unwrapped[mask]\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(freqs, np.abs(cross_spec))\n",
    "plt.xlim([0, 4])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(freqs, phase, label='Phase')\n",
    "plt.plot(freqs, phase_unwrapped, label='Unwrapped Phase')\n",
    "\n",
    "# Plot red dotted lines for each frequency in freqs_used\n",
    "for i, f_used in enumerate(freqs_used):\n",
    "    if i == 0: # Add label only for the first line to avoid cluttering the legend\n",
    "        plt.axvline(x=f_used, color='red', linestyle='dotted', label='Freqs Used')\n",
    "    else:\n",
    "        plt.axvline(x=f_used, color='red', linestyle='dotted')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.xlim([0,4])\n",
    "plt.ylim([-2*np.pi, 2*np.pi])\n",
    "plt.legend()\n",
    "\n",
    "if len(freqs_used) > 0:\n",
    "    # Fit a line to phase_used vs freqs_used\n",
    "    coeffs = np.polyfit(freqs_used, phase_used, 1)  # returns [slope, intercept]\n",
    "    slope = coeffs[0]\n",
    "    tau_est = -slope / (2*np.pi)\n",
    "else:\n",
    "    # If no frequencies are used, estimate tau from the first frequency and pha\n",
    "    tau_est = phase_used[0]/(2*np.pi*freqs_used[0])\n",
    "\n",
    "print(f\"Frequency bins: {freqs_used}Hz, \\nPhase: {phase_used}, \\nEstimated time delay: {tau_est*1000:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Cross-Correlation and Phase Transform (GCC-PHAT)\n",
    "\n",
    "The Generalized Cross-Correlation (GCC) framework is a family of techniques that modify the cross-correlation by applying a frequency-domain weighting to emphasize certain signal characteristics. The standard cross-correlation is actually one member of this family (with a flat frequency weight). By choosing a clever weight, one can improve the detection of the correlation peak in noisy or reverberant conditions.\n",
    "\n",
    "One popular choice is the Phase Transform (PHAT) weighting. In GCC-PHAT, we whiten the spectrum by dividing by the magnitude of the cross-spectrum. The PHAT-weighted cross-correlation is computed as:\n",
    "\n",
    "$$\n",
    "R_{\\text{PHAT}}(\\tau) = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} \\frac{X(\\omega)Y^*(\\omega)}{|X(\\omega)Y^*(\\omega)|} \\, e^{j\\omega\\tau}\\,d\\omega\\,.\n",
    "$$\n",
    "\n",
    "Intuitively, we keep only the phase of each frequency’s contribution (hence “Phase Transform”) and ignore the magnitude. This has the effect of giving equal weight to all frequencies, so that no single frequency (or narrow band) dominates the correlation. If $y(t)$ is exactly $x(t-\\tau_0)$ (delayed copy), and there is no other distortion, then $X(\\omega)Y^*(\\omega) = |X(\\omega)|^2 e^{-j\\omega\\tau_0}$. Plugging into the PHAT formula yields:\n",
    "\n",
    "$$\n",
    "R_{\\text{PHAT}}(\\tau) = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} e^{-j\\omega\\tau_0} e^{j\\omega\\tau}\\,d\\omega = \\delta(\\tau - \\tau_0)\\,,\n",
    "$$\n",
    "\n",
    "an ideal delta-function peak at the true delay. In discrete time, we can’t get an infinite resolution delta, but we do get a very sharp peak concentrated at the correct sample (and possibly fractional lag after interpolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute FFT of signals (zero-pad to length N for efficient convolution length)\n",
    "N = len(x) + len(y) - 1\n",
    "X = np.fft.rfft(x, n=N)\n",
    "Y = np.fft.rfft(y, n=N)\n",
    "freqs = np.fft.rfftfreq(N, d=1/hs_fps)\n",
    "C = X * np.conj(Y)\n",
    "# Apply PHAT weighting\n",
    "EPS = 1e-6\n",
    "# C_phat = C   # add small EPS to avoid /0\n",
    "C_phat = C / (np.abs(C) + EPS)   # add small EPS to avoid /0\n",
    "gcc_phat = np.fft.irfft(C_phat, n=N)\n",
    "gcc_phat = np.roll(gcc_phat, N//2)  # shift zero lag to center\n",
    "# Now gcc_phat is the cross-correlation (PHAT weighted)\n",
    "lag_index = np.argmax(gcc_phat)\n",
    "tau_samples = lag_index - (len(x)-1)  # convert index to lag (center corresponds to 0 lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcc_0(x, y, fs, tau_, PHAT_weight = False):\n",
    "    # N = len(x) + len(y) - 1\n",
    "    N = len(x)\n",
    "    fs = hs_fps\n",
    "    X = np.fft.rfft(x, n=N)\n",
    "    Y = np.fft.rfft(y, n=N)\n",
    "    freqs = np.fft.rfftfreq(N, d=1/fs)\n",
    "    C = X * np.conj(Y)*np.exp(1j*2*np.pi*freqs*tau_)\n",
    "    \n",
    "    # Apply PHAT weighting\n",
    "    if PHAT_weight:\n",
    "        EPS = 1e-6\n",
    "        C_phat = C / (np.abs(C) + EPS)   # add small EPS to avoid /0\n",
    "    else:\n",
    "        C_phat = C   # add small EPS to avoid /0\n",
    "    gcc_phat = np.fft.irfft(C_phat, n=N)\n",
    "    # gcc_phat = np.roll(gcc_phat, N//2)  # shift zero lag to center\n",
    "    # Now gcc_phat is the cross-correlation (PHAT weighted)\n",
    "    # return np.max(gcc_phat)\n",
    "    return gcc_phat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = np.linspace(-.1, 0.1, 1000)\n",
    "gcc_tau = list()\n",
    "for tau_ in tau_list:\n",
    "    gcc_tau.append(gcc_0(x, y, hs_fps, tau_, True))\n",
    "print(f\"Estimated tau: {tau_list[np.argmax(gcc_tau)]*1000:.3f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = np.linspace(-.1, 0.1, 1000)\n",
    "gcc_tau = list()\n",
    "for tau_ in tau_list:\n",
    "    gcc_tau.append(gcc_0(x, y, hs_fps, tau_))\n",
    "print(f\"Estimated tau: {tau_list[np.argmax(gcc_tau)]*1000:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspace Methods (MUSIC and ESPRIT)\n",
    "Subspace methods like MUSIC (Multiple Signal Classification) and ESPRIT were originally developed for direction-of-arrival estimation on sensor arrays, but they can be adapted for time-delay estimation to achieve even higher resolution than classical methods​. The idea is to model the signals and noise and then use eigen-decomposition to separate the “signal subspace” (which contains the delayed copies of the waveform) from the “noise subspace.” These methods are especially powerful when multiple propagation paths (multiple delays) exist, as they can resolve distinct delays that are closer than a sample period (super-resolution). How it works (conceptually for a single delay): If $y(t)$ is a delayed version of $x(t)$, in the frequency domain we have $Y(f) = X(f)e^{-j2\\pi f \\tau}$. We can treat each frequency component as a sensor observing the phase-shifted signal. For example, take a set of $M$ frequency bins ${f_1, f_2, ..., f_M}$ where the signal has significant energy. The phase of $Y$ relative to $X$ at those bins is $-2\\pi f_k \\tau$. This forms a model akin to an array of $M$ sensors (frequencies) with a single source at “angle” (delay) $\\tau$. MUSIC can be applied by constructing a covariance matrix of the cross-spectral data across these $M$ “sensors.” The covariance will have a rank-1 signal subspace (for one delay) plus noise. MUSIC then scans a candidate $\\tau$ and evaluates a pseudo-spectrum $P(\\tau) = \\frac{1}{\\mathbf{a}^H(\\tau) \\mathbf{N} \\mathbf{N}^H \\mathbf{a}(\\tau)}$ where $\\mathbf{N}$ is the noise subspace eigenvectors and $\\mathbf{a}(\\tau)$ is the array steering vector $[1, e^{-j2\\pi f_2 \\tau}, ..., e^{-j2\\pi f_M \\tau}]^T$. Peaks of $P(\\tau)$ indicate likely delays (where the candidate steering vector aligns with the signal subspace)​. ESPRIT, on the other hand, can find the delays algebraically by exploiting invariances (often requiring paired sensors or frequencies). In the time domain, one can also create multiple “sensors” by taking delayed versions of the signal (e.g., constructing a covariance matrix from $[x(t), y(t)]$ as two sensors, or using lagged versions as additional sensors via covariance lags or sub-band averaging​). In ultrawideband scenarios or radar with multiple echoes, one can often partition the spectrum or time into sub-bands which act as multiple measurements for the subspace algorithm​. These details can get mathematically heavy, but the end result is an estimate of one or more delays with very high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x)\n",
    "X = np.fft.rfft(x,N)\n",
    "Y = np.fft.rfft(y,N)\n",
    "freqs = np.fft.rfftfreq(N, 1/hs_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X*Y.conj()\n",
    "R = np.outer(Z, Z.conj())\n",
    "eigvals, eigvecs = np.linalg.eigh(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "En = eigvecs[:, :-1]\n",
    "delay_grid = np.linspace(-.1, .1, 500)\n",
    "# delay_grid = np.linspace(-0.1, 0.1, 100)\n",
    "pseudospectrum = []\n",
    "for tau_ in delay_grid:\n",
    "    a = np.exp(-1j * 2 * np.pi * tau_ * freqs)\n",
    "    a = a[:, np.newaxis]\n",
    "    denom = np.abs(a.conj().T @ En @ En.conj().T @ a)[0,0]\n",
    "    pseudospectrum.append(1/denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delay_grid[np.argmax(pseudospectrum)])\n",
    "plt.plot(delay_grid, np.array(pseudospectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.exp(-1j * 2 * np.pi * tau_ * freqs)\n",
    "a_abs = a*np.abs(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "En = eigvecs[:, -1:]\n",
    "# delay_grid = np.linspace(-1, 1, 100)\n",
    "delay_grid = np.linspace(-0.1, 0.1, 100)\n",
    "pseudospectrum = []\n",
    "for tau_ in delay_grid:\n",
    "    a = np.exp(-1j * 2 * np.pi * tau_ * freqs)\n",
    "    a = a[:, np.newaxis]\n",
    "    denom = np.abs(a.conj().T @ En @ En.conj().T @ a)[0,0]\n",
    "    pseudospectrum.append(denom)\n",
    "\n",
    "print(delay_grid[np.argmax(pseudospectrum)])\n",
    "plt.plot(delay_grid, np.array(pseudospectrum))\n",
    "\n",
    "pseudospectrum = []\n",
    "for tau_ in delay_grid:\n",
    "    a = np.exp(-1j * 2 * np.pi * tau_ * freqs)*np.abs(Z)\n",
    "    a = a[:, np.newaxis]\n",
    "    denom = np.abs(a.conj().T @ En @ En.conj().T @ a)[0,0]\n",
    "    pseudospectrum.append(denom)\n",
    "\n",
    "print(delay_grid[np.argmax(pseudospectrum)])\n",
    "plt.plot(delay_grid, np.array(pseudospectrum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-band MUSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUSIC applied on selected frequency bins\n",
    "def music_on_selected_band(x, y, fs, freq_band, delay_grid):\n",
    "    N = len(x)\n",
    "    X = np.fft.rfft(x)\n",
    "    Y = np.fft.rfft(y)\n",
    "    freqs = np.fft.rfftfreq(N, d=1/fs)\n",
    "    # Select frequency bins\n",
    "    bin_mask = (freqs >= freq_band[0]) & (freqs <= freq_band[1])\n",
    "    X_sel = X[bin_mask]\n",
    "    Y_sel = Y[bin_mask]\n",
    "    freqs_sel = freqs[bin_mask]\n",
    "    # Cross-spectrum\n",
    "    Z = X_sel * np.conj(Y_sel)\n",
    "    R = np.outer(Z, Z.conj())\n",
    "    # Eigen-decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(R)\n",
    "    En = eigvecs[:, :-1]  # assume 1 delay\n",
    "    # MUSIC pseudospectrum\n",
    "    pseudospectrum = []\n",
    "    for tau in delay_grid:\n",
    "        a = np.exp(-1j * 2 * np.pi * freqs_sel * tau)[:, np.newaxis]\n",
    "        denom = np.abs(a.conj().T @ En @ En.conj().T @ a)[0, 0]\n",
    "        pseudospectrum.append(1 / denom)\n",
    "    return np.real(pseudospectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_grid = np.linspace(-.1, .1, 500)\n",
    "freq_band = [0.7, 2.0]\n",
    "# freq_band = [1.4, 1.5]\n",
    "fs = hs_fps\n",
    "pseudo_spectrum_narrow_band = music_on_selected_band(x, y, fs, freq_band, delay_grid)\n",
    "print(delay_grid[np.argmax(pseudo_spectrum_narrow_band)])\n",
    "plt.plot(delay_grid, pseudo_spectrum_narrow_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark on Thai HS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Delay Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Import the module\n",
    "time_delay_estimation = importlib.import_module('time_delay_estimation')\n",
    "\n",
    "# Reload the module to ensure any changes are applied\n",
    "time_delay_estimation = importlib.reload(time_delay_estimation)\n",
    "\n",
    "# Define the list of function names\n",
    "delay_estimation_methods = ['frac_delay', 'sinc_interp', 'gcc_delay']\n",
    "\n",
    "# Create a dictionary of functions using dictionary comprehension\n",
    "delay_estimation_functions = {\n",
    "    method: getattr(time_delay_estimation, method) \n",
    "    for method in delay_estimation_methods\n",
    "}\n",
    "\n",
    "# Setting parameters for the delay estimation\n",
    "tau_grid = np.linspace(-.1,.1, 1000)\n",
    "scale_factor = 16\n",
    "PHAT_weight = False\n",
    "\n",
    "fs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy_from_pkl(pkl_path, hs_fps=150):\n",
    "    roi_df = np.load(pkl_path, allow_pickle=True)\n",
    "    x = roi_df['roi-forehead'].to_list()\n",
    "    y = roi_df['roi-right_cheek'].to_list()\n",
    "    ts = roi_df['timestamp'].to_list()\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    ts = np.array(ts)\n",
    "    ts-=ts[0]\n",
    "\n",
    "    x = x[:,[2,1,0]] # BGR -> RGB\n",
    "    y = y[:,[2,1,0]]\n",
    "\n",
    "    x = CHROM_win(x[:,None,:], FS=hs_fps)\n",
    "    y = CHROM_win(y[:,None,:], FS=hs_fps)\n",
    "    x = x[0]\n",
    "    y = y[0]\n",
    "    return (x, y), ts\n",
    "\n",
    "def load_roi_from_pkl(pkl_path, hs_fps=150):\n",
    "    roi_df = np.load(pkl_path, allow_pickle=True)\n",
    "    roi_list = ['forehead', 'glabella', 'left_cheek', 'right_cheek', 'chin']\n",
    "    roi_dict = {}\n",
    "    for roi_ in roi_list:\n",
    "        roi_arr = np.array(roi_df['roi-forehead'].to_list())\n",
    "        roi_arr = roi_arr[:,[2,1,0]] # BGR -> RGB\n",
    "        roi_arr = CHROM_win(roi_arr[:,None,:], FS=hs_fps)[0]\n",
    "        roi_dict[roi_] = roi_arr\n",
    "    return roi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from module_ptt_estimator.extractor import PTTExtractor\n",
    "\n",
    "roi_path = r\"C:\\Users\\Shootall\\Documents\\rPPG-dataset\\highspeed_roi\"\n",
    "bp_path = r\"C:\\Users\\Shootall\\Documents\\rPPG-dataset\\highspeed_bp\\highspeed_bp\"\n",
    "roi_file_list = glob(os.path.join(roi_path, \"*.pkl\"))\n",
    "bp_file_list = glob(os.path.join(bp_path, \"*.pkl\"))\n",
    "\n",
    "pttExt = PTTExtractor()\n",
    "\n",
    "delay_df = defaultdict(list)\n",
    "\n",
    "for f_ in roi_file_list:\n",
    "    (x, y), ts = load_xy_from_pkl(f_, hs_fps=fs)\n",
    "    match_name = re.search('sid(\\d+)-rid(\\d+)-qhy5iii174c-seg(\\d+)-roi.pkl', f_).groups()\n",
    "    sid, rid, i_seg = match_name\n",
    "    print(f\"Working on {match_name}\")\n",
    "\n",
    "    delay_df['sid'].append(sid)\n",
    "    delay_df['rid'].append(rid)\n",
    "    delay_df['i_seg'].append(i_seg)\n",
    "\n",
    "    bp_df = np.load(os.path.join(bp_path, f\"sid{sid}-rid{rid}-uscom_bp_plus-seg{i_seg}-reference.pkl\"), allow_pickle=True)\n",
    "    if bp_df.shape[0] > 0:\n",
    "        delay_df['sbp'].append(bp_df.iloc[0]['Log_Sys'])\n",
    "        delay_df['dbp'].append(bp_df.iloc[0]['Log_Dia'])\n",
    "    else:\n",
    "        delay_df['sbp'].append(np.nan)\n",
    "        delay_df['dbp'].append(np.nan)\n",
    "\n",
    "    for de_method in delay_estimation_methods:\n",
    "        delay = delay_estimation_functions[de_method](x, y, fs, scale_factor=scale_factor, tau_grid=tau_grid, PHAT_weight=PHAT_weight)\n",
    "        # print(f\"{de_method}: {delay:.4f} seconds\")\n",
    "        delay_df[de_method].append(delay)\n",
    "    pai_ptt_peak = pttExt.ptt_via_peak2peak(ts, x, ts, y,)\n",
    "    delay_df['pai_ptt_peak'].append(np.mean(pai_ptt_peak['valid_ptt_list']))\n",
    "    pai_ptt_slope = pttExt.ptt_via_slope2slope(ts, x, ts, y)\n",
    "    delay_df['pai_ptt_slope'].append(np.mean(pai_ptt_slope['valid_ptt_list']))\n",
    "    pai_ptt_corr = pttExt.ptt_via_slope2slope(ts, x, ts, y)\n",
    "    delay_df['pai_ptt_corr'].append(np.mean(pai_ptt_corr['valid_ptt_list']))\n",
    "\n",
    "delay_df = pd.DataFrame(delay_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>rid</th>\n",
       "      <th>i_seg</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>frac_delay</th>\n",
       "      <th>sinc_interp</th>\n",
       "      <th>gcc_delay</th>\n",
       "      <th>pai_ptt_peak</th>\n",
       "      <th>pai_ptt_slope</th>\n",
       "      <th>pai_ptt_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.013950</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.055883</td>\n",
       "      <td>0.073396</td>\n",
       "      <td>0.073396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.059281</td>\n",
       "      <td>0.034645</td>\n",
       "      <td>0.034645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.013224</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>0.056898</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.041237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>0.051828</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>0.023661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.021321</td>\n",
       "      <td>0.057837</td>\n",
       "      <td>0.037856</td>\n",
       "      <td>0.037856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.066696</td>\n",
       "      <td>0.040558</td>\n",
       "      <td>0.040558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>0.079186</td>\n",
       "      <td>0.063252</td>\n",
       "      <td>0.063252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.081568</td>\n",
       "      <td>0.067598</td>\n",
       "      <td>0.067598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>-0.012917</td>\n",
       "      <td>-0.013113</td>\n",
       "      <td>0.063910</td>\n",
       "      <td>0.049918</td>\n",
       "      <td>0.049918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-0.014964</td>\n",
       "      <td>-0.015000</td>\n",
       "      <td>-0.014915</td>\n",
       "      <td>0.053558</td>\n",
       "      <td>0.075586</td>\n",
       "      <td>0.075586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1367 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sid rid i_seg    sbp   dbp  frac_delay  sinc_interp  gcc_delay  \\\n",
       "0     100   1     1  148.0  93.0    0.013950     0.013750   0.013914   \n",
       "1     100   1     2  148.0  93.0    0.009128     0.009167   0.009109   \n",
       "2     100   1     3  148.0  93.0    0.013224     0.013333   0.013313   \n",
       "3     100   1     4  148.0  93.0    0.017688     0.017500   0.017718   \n",
       "4     100   1     5  148.0  93.0    0.021274     0.021250   0.021321   \n",
       "...   ...  ..   ...    ...   ...         ...          ...        ...   \n",
       "1362   99   3     1  126.0  87.0    0.001326     0.001250   0.001301   \n",
       "1363   99   3     2  126.0  87.0   -0.000946    -0.000833  -0.000901   \n",
       "1364   99   3     3  126.0  87.0    0.008013     0.007917   0.008108   \n",
       "1365   99   3     4  126.0  87.0   -0.013053    -0.012917  -0.013113   \n",
       "1366   99   3     5  126.0  87.0   -0.014964    -0.015000  -0.014915   \n",
       "\n",
       "      pai_ptt_peak  pai_ptt_slope  pai_ptt_corr  \n",
       "0         0.055883       0.073396      0.073396  \n",
       "1         0.059281       0.034645      0.034645  \n",
       "2         0.056898       0.041237      0.041237  \n",
       "3         0.051828       0.023661      0.023661  \n",
       "4         0.057837       0.037856      0.037856  \n",
       "...            ...            ...           ...  \n",
       "1362      0.066696       0.040558      0.040558  \n",
       "1363      0.079186       0.063252      0.063252  \n",
       "1364      0.081568       0.067598      0.067598  \n",
       "1365      0.063910       0.049918      0.049918  \n",
       "1366      0.053558       0.075586      0.075586  \n",
       "\n",
       "[1367 rows x 11 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.93435822650756"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.mean(np.diff(ts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
